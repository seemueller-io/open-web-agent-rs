name: Rust

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

env:
  CARGO_TERM_COLOR: always

jobs:
  # Job for building on macOS (with Metal)
  build-linux:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: Jimver/cuda-toolkit@v0.2.24
        id: cuda-toolkit
        with:
          cuda: '12.5.0'
      - name: Setup Python virtual environment
        shell: bash
        run: |
          python3 -m venv .venv
          source .venv/bin/activate
          python3 -m pip install --upgrade pip
          python3 -m pip install huggingface-hub
      - name: Login to HuggingFace Hub
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
        run: |
          source .venv/bin/activate
          if [ -n "$HF_TOKEN" ]; then
            echo "Logging into Hugging Face Hub..."
            huggingface-cli login --token $HF_TOKEN
          else
            echo "No HuggingFace token found, skipping login"
          fi
      - uses: oven-sh/setup-bun@v2
      - run: bun install
      # If you intend to use CUDA on Linux (highly recommended for performance with Candle)
      # You'll need a runner with GPU support or use an action that sets up CUDA.
      # GitHub's default ubuntu-latest runners are CPU-only.
      # For GPU runners, you'd typically use self-hosted runners or a service like vast.ai.
      # For CI, you might build a CPU-only version for faster checks, or use a GPU-enabled runner for full testing.

      # Option 1: Build with CPU backend (no CUDA, no Metal)
      - name: Build (Linux - CPU only)
        run: cargo build --release --no-default-features

      # Option 2: Build with CUDA backend (requires CUDA setup and often a GPU runner)
      # - name: Setup CUDA (example - specific action might vary)
      #   uses: jimmy-shian/setup-cuda@v1 # This is just an example, research appropriate action
      #   with:
      #     cuda-version: '12.0' # Or your desired CUDA version
      # - name: Build (Linux - CUDA)
      #   # Explicitly enable cudarc and candle-core's cuda feature, and potentially intel-mkl-src
      #   run: cargo build --release --features "cudarc candle-core/cuda intel-mkl-src"

      - name: Run tests (Linux - CPU only)
        # If your tests are CPU-only, they'll work with the CPU build.
        # If they require CUDA, they'd need the CUDA build and runner.
        run: cargo test --no-default-features

  build-macos:
    runs-on: macos-latest



    steps:
      - uses: actions/checkout@v4
      - name: Setup Python virtual environment
        shell: bash
        run: |
          python3 -m venv .venv
          source .venv/bin/activate
          python3 -m pip install --upgrade pip
          python3 -m pip install huggingface-hub
      - name: Login to HuggingFace Hub
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
        run: |
          source .venv/bin/activate
          if [ -n "$HF_TOKEN" ]; then
            echo "Logging into Hugging Face Hub..."
            huggingface-cli login --token $HF_TOKEN
          else
            echo "No HuggingFace token found, skipping login"
          fi
      - uses: oven-sh/setup-bun@v2
      - run: bun install
      - name: Build (macOS - Metal)
        # The `metal` feature will be automatically picked up due to target_os = "macos" in Cargo.toml
        run: cargo build --release
      - name: Run tests (macOS)
        run: cargo test
